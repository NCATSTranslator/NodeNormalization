[![Build Status](https://travis-ci.com/TranslatorIIPrototypes/NodeNormalization.svg?branch=master)](https://travis-ci.com/TranslatorIIPrototypes/NodeNormalization)

# NodeNormalization

## Introduction

Node normalization takes a CURIE, and returns:

* The preferred CURIE for this entity
* All other known equivalent identifiers for the entity
* Semantic types for the entity as defined by the [Biolink Model](https://biolink.github.io/biolink-model/)

The data currently served by Node Normalization is created by the prototype project
[Babel](https://github.com/TranslatorSRI/Babel), which attempts to find identifier equivalences,
and makes sure that CURIE prefixes are BioLink Model compliant.  The NodeNormalization service, however,
is independent of Babel and as improved identifier equivalence tools are developed, their results
can be easily incorporated.

To determine whether Node Normalization is likely to be useful, check /get_semantic_types, which lists the BioLink
semantic types for which normalization has been attempted, and /get_curie_prefixes,
which lists the number of times each prefix is used for a semantic type.

For examples of service usage, see the example [notebook](documentation/NodeNormalization.ipynb).

The Node normalization website leverages the [R3 (Redis-REST with referencing)](https://github.com/TranslatorSRI/r3) Redis data design and configuration. 

Users can find the publicly available website at [service](https://nodenormalization-sri.renci.org/docs).

## Installation

Create a virtual environment
```shell
python -m venv nodeNormalization-env
```
Activate the virtual environment
```shell
source nodeNormalization-env/bin/activate
```
Install requirements 
```shell
pip install -r requirements.txt
```
## Generating equivalence data

The equivalence data can be generated by running [Babel](https://github.com/TranslatorSRI/Babel). An example of the contents of a compendia file is shown below:
```json lines
{"id": {"identifier": "PUBCHEM:50986940"}, "equivalent_identifiers": [{"identifier": "PUBCHEM:50986940"}, {"identifier": "INCHIKEY:CYMOSKLLKPIPCD-UHFFFAOYSA-N"}], "type": ["chemical_substance", "named_thing", "biological_entity", "molecular_entity"]}
{"id": {"identifier": "CHEMBL.COMPOUND:CHEMBL1546789", "label": "CHEMBL1546789"}, "equivalent_identifiers": [{"identifier": "CHEMBL.COMPOUND:CHEMBL1546789", "label": "CHEMBL1546789"}, {"identifier": "PUBCHEM:4879549"}, {"identifier": "INCHIKEY:FUIYIXDZTPMQEH-UHFFFAOYSA-N"}], "type": ["chemical_substance", "named_thing", "biological_entity", "molecular_entity"]}
```
## Starting a local instance

NodeNormalizer can be started locally using Docker Compose:
```shell
docker compose -f docker-compose.yml up --build
```
Then navigate to http://localhost:8080/docs to see available endpoints.

## Loading Data
Loading the test data is quick & easy using Docker Exec for both compendia & conflation files:
```shell
docker container exec -it node-norm python node_normalizer/load_compendium.py -c tests/resources/Cell.txt -c tests/resources/Disease.txt -c tests/resources/Gene.txt -c tests/resources/Protein.txt -r redis_config.yaml
docker container exec -it node-norm python node_normalizer/load_conflation.py -c tests/resources/GeneProtein.txt -s gene_protein -r redis_config.yaml
```

Note that the dataset for Node normalization is quite large and 256Gb of memory and disk space should be made available to the Redis instance to ensure proper loading of the all compendia files.

## Useful Redis commands

It is possible to observer the progress of the load opening a command line _within the container_ and issuing Redis commands.

_View the number of keys loaded so far._
 ```shell
redis-cli info keyspace
```

_Once the database has completed loading it is recommended that the Redis database be persisted to disk._
```shell
redis-cli save
```

_Monitor the database to determine if the save has completed._
```shell
redis-cli info persistence
```

### Kubernetes configurations
Kubernetes configurations and helm charts for this project can be found at: 
```
https://github.com/helxplatform/translator-devops/helm/r3
```  

## Running the Tests
The tests can be run with the following command:
```shell
pytest -svv tests
```
Note that the tests leverage Docker, so ensure that no running instance of the NodeNormalizer is already running.